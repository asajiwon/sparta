import streamlit as st
import openai
import os
import sys
import json
import pdfplumber
import docx
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional
import uvicorn
from dotenv import load_dotenv

# ====== í™˜ê²½ ì„¤ì • ======
sys.stdout.reconfigure(encoding='utf-8')
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ====== ì´ë ¥ì„œ íŒŒì‹± ======
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return clean_text(text)

def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    text = "\n".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])
    return clean_text(text)

def extract_resume_text(file_path):
    ext = os.path.splitext(file_path)[-1].lower()
    if ext == ".pdf":
        return extract_text_from_pdf(file_path)
    elif ext == ".docx":
        return extract_text_from_docx(file_path)
    else:
        raise ValueError(f"Unsupported file format: {ext}")

def clean_text(text):
    lines = text.split("\n")
    cleaned_lines = [line.strip() for line in lines if line.strip()]
    return "\n".join(cleaned_lines)

# ====== GPT í”„ë¡¬í”„íŠ¸ ë¹Œë” ======
def build_gpt_prompt(user_message, resume_text):
    prompt = f"""...
[ì‚¬ìš©ì ììœ  ì…ë ¥]
{user_message}

[ì´ë ¥ì„œ í…ìŠ¤íŠ¸]
{resume_text}
"""
    return prompt

def build_strategy_prompt(primary_json):
    return f"""...
"""

async def analyze_user_message(user_message, resume_text):
    primary_prompt = build_gpt_prompt(user_message, resume_text)

    primary_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” í˜„ì‹¤ì ì¸ ì»¤ë¦¬ì–´ ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
            {"role": "user", "content": primary_prompt}
        ],
        temperature=0.2,
        max_tokens=1500,
    )
    primary_result_text = primary_response.choices[0].message.content
    cleaned_primary_text = primary_result_text.replace("```json", "").replace("```", "").strip()
    primary_result_json = json.loads(cleaned_primary_text)

    secondary_prompt = build_strategy_prompt(primary_result_json)
    secondary_response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” í˜„ì‹¤ì ì¸ ì»¤ë¦¬ì–´ ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
            {"role": "user", "content": secondary_prompt}
        ],
        temperature=0.2,
        max_tokens=1500,
    )
    secondary_result_text = secondary_response.choices[0].message.content
    cleaned_secondary_text = secondary_result_text.replace("```json", "").replace("```", "").strip()
    secondary_result_json = json.loads(cleaned_secondary_text)

    return secondary_result_json

# ====== FastAPI ì„œë²„ ì„¤ì • ======
app = FastAPI()

class AnalysisResult(BaseModel):
    result_json: dict

@app.post("/analyze", response_model=AnalysisResult)
async def analyze(user_message: str = Form(...), resume: Optional[UploadFile] = File(None)):
    try:
        resume_text = ""
        if resume is not None and resume.filename != "":
            file_location = f"temp_{resume.filename}"
            with open(file_location, "wb") as f:
                f.write(await resume.read())
            resume_text = extract_resume_text(file_location)
            os.remove(file_location)

        result_json = await analyze_user_message(user_message, resume_text)
        return AnalysisResult(result_json=result_json)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ====== Streamlit ì•± ======
st.set_page_config(page_title="íŒ©íŠ¸í­ê²©ë© í‡´ì‚¬ì§„ë‹¨ê¸° ğŸ”¥", page_icon="ğŸ”¥", layout="centered")
st.title("íŒ©íŠ¸í­ê²©ë© í‡´ì‚¬ì§„ë‹¨ê¸° ğŸ”¥")
st.subheader("í‡´ì‚¬ ê³ ë¯¼? íŒ©íŠ¸ë¶€í„° ì¡°ì ¸ë³´ì.")

if "step" not in st.session_state:
    st.session_state.step = 1
if "user_answers" not in st.session_state:
    st.session_state.user_answers = []

def ask_question(prompt, key):
    st.write(prompt)
    return st.text_input("ëŒ€ë‹µ:", key=key)

if st.session_state.step == 1:
    answer = ask_question("í‡´ì‚¬ ê³ ë¯¼ ìˆì§€? ì™œ ê·¸ë ‡ê²Œ ë¹¡ì³¤ëƒ.", "first_answer")
    if answer:
        st.session_state.user_answers.append(answer)
        st.session_state.step = 2
elif st.session_state.step == 2:
    answer = ask_question("ì˜¤ì¼€ì´. ë‹ˆ ê²½ë ¥ ëª‡ ë…„ ì°¨.", "second_answer")
    if answer:
        st.session_state.user_answers.append(answer)
        st.session_state.step = 3
elif st.session_state.step == 3:
    answer = ask_question("ì§€ê¸ˆê¹Œì§€ ë¬´ìŠ¨ ì¼ í–ˆëƒ. ì§§ê²Œ.", "third_answer")
    if answer:
        st.session_state.user_answers.append(answer)
        st.session_state.step = 4
elif st.session_state.step == 4:
    answer = ask_question("ì¨ë³¸ íˆ´, ê¸°ìˆ  ë­ ìˆëƒ.", "forth_answer")
    if answer:
        st.session_state.user_answers.append(answer)
        st.session_state.step = 5
elif st.session_state.step == 5:
    answer = ask_question("ì—°ë´‰ì€ ì–¼ë§ˆ ë°›ê³  ì‹¶ëƒ.", "fifth_answer")
    if answer:
        st.session_state.user_answers.append(answer)
        st.session_state.step = 6
elif st.session_state.step == 6:
    if st.button("íŒ©í­ ë¶„ì„ ì‹œì‘í•˜ê¸° âš¡"):
        with st.spinner("íŒ©í­ ë¶„ì„ ì¤‘..."):
            try:
                user_message = "\n".join(st.session_state.user_answers)
                prompt = f"""
[ì§€ì¹¨]
- '1. 2. 3.' ê°™ì€ ëª©ë¡ ë‚˜ì—´ ì ˆëŒ€ ê¸ˆì§€.
- ì‚¬ëŒ ë§íˆ¬ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë˜, ì§§ê²Œ ëŠëŠ” ë¬¸ì¥ìœ¼ë¡œ íŒ©íŠ¸í­ê²©í•´.
- ê°ì • ìœ„ë¡œë‚˜ ì°©í•œ ë§íˆ¬ ì ˆëŒ€ ì“°ì§€ ë§ˆë¼. ì¹œì ˆ âŒ, ì‹¸ëŠ˜í•˜ê²Œ âŒ, ë¬´ì‹¬í•˜ê²Œ íŒ©íŠ¸ë§Œ ë§í•´.
- ì´ëª¨í‹°ì½˜ ê¸ˆì§€.
- í˜„ì‹¤ ì§ê²©: ê²½ë ¥ê³¼ ìŠ¤í‚¬ì„ ëƒ‰ì •í•˜ê²Œ í‰ê°€í•˜ê³ , ì‹œì¥ ìƒí™©ì„ ê¹”ë”í•˜ê²Œ ë°•ì‚´ë‚´ê³ , ëì— í˜„ì‹¤ ì¡°ì–¸ í•œ ì¤„ë¡œ ì •ë¦¬í•´.
- ê°€ëŠ¥í•˜ë©´ í™•ë¥ (%)ì´ë‚˜ ì˜ˆìƒ ê¸°ê°„(ê°œì›”) ê°™ì€ ìˆ«ìë„ ìì—°ìŠ¤ëŸ½ê²Œ ì„ì–´ì¤˜.

[ì°¸ê³  ì˜ˆì‹œ ìŠ¤íƒ€ì¼]
â€œí”„ë¡œê·¸ë˜ë¨¸ë¡œ 5ë…„ ì¼í–ˆë‹¤ê³ ? ì½”ë“œ í€„ë¦¬í‹° ë³´ë‹ˆê¹Œ ìœ ì§€ë³´ìˆ˜ ê±±ì • ìƒê¸´ë‹¤. ëŒ€ì¶© ë„˜ê¸¸ ìŠ¤í™ ì•„ë‹˜.
ì§€ê¸ˆ ê°œë°œì ì‹œì¥? ê³µê³  í•˜ë‚˜ì— ì§€ì›ì ìˆ˜ë°± ëª… ëª°ë¦°ë‹¤. ë‹ˆ ìŠ¤í‚¬ì…‹ìœ¼ë¡  ê·¸ëƒ¥ ë¬»íŒë‹¤.
ì‹ ê·œ í”„ë¡œì íŠ¸ ë“¤ì–´ê°€ê³  ì‹¶ìœ¼ë©´, ì½”ë“œë¶€í„° ë‹¤ì‹œ ë‹¦ê³  ìµœì‹  íŠ¸ë Œë“œ ë”°ë¼ì¡ì•„. í•  ìì‹  ì—†ìœ¼ë©´ ê·¸ëƒ¥ í˜„ìƒìœ ì§€ë‚˜ í•´."

[ë¶„ì„í•  ìœ ì € ì •ë³´]
{user_message}
"""
                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ìœ ì € ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ëƒ‰ì² í•œ í”¼ë“œë°±ì„ ì‘ì„±í•´"},
                        {"role": "user", "content": prompt}
                    ]
                )
                llm_answer = response.choices[0].message.content
                st.write(llm_answer)

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# ====== FastAPI ì‹¤í–‰ ======
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8888, reload=True)
